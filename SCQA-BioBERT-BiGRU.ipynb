{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edcKb6Dr-Yf4"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:37:49.654198Z",
     "iopub.status.busy": "2025-11-10T05:37:49.653866Z",
     "iopub.status.idle": "2025-11-10T05:38:19.721401Z",
     "shell.execute_reply": "2025-11-10T05:38:19.720646Z",
     "shell.execute_reply.started": "2025-11-10T05:37:49.654165Z"
    },
    "id": "30hMmlk3xIh1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import re\n",
    "import random\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-L7iiUI-75Q"
   },
   "source": [
    "# Config, Load & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:23.571873Z",
     "iopub.status.busy": "2025-11-10T05:38:23.571688Z",
     "iopub.status.idle": "2025-11-10T05:38:29.583652Z",
     "shell.execute_reply": "2025-11-10T05:38:29.582901Z",
     "shell.execute_reply.started": "2025-11-10T05:38:23.571858Z"
    },
    "id": "3dYgNaTTaJV5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the 12 categories\n",
    "CATEGORIES = ['Pathology', 'Anatomy', 'Pharmacology', 'Microbiology', 'Gynaecology & Obstetrics', 'Dental', 'Physiology',\n",
    "              'Biochemistry', 'Pediatrics', 'Ophthalmology', 'Psychiatry', 'Radiology']\n",
    "\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "try:\n",
    "    data_path = \"data/\" \n",
    "    \n",
    "    train_df_raw = pd.read_json(data_path + \"train.json\", lines=True)\n",
    "    dev_df_raw = pd.read_json(data_path + \"dev.json\", lines=True)\n",
    "    test_df_raw = pd.read_json(data_path + \"test.json\", lines=True)\n",
    "    print(f\"Loaded raw data: train({len(train_df_raw)}), dev({len(dev_df_raw)}), test({len(test_df_raw)})\\\")\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON files: {e}\")\n",
    "\n",
    "\n",
    "def parse_correct_answers(row):\n",
    "\n",
    "    correct_answers = [0, 0, 0, 0]\n",
    "    exp_text = str(row['exp']) \n",
    "\n",
    "    if pd.notnull(exp_text):\n",
    "\n",
    "        correct_indices = re.findall(r'[.\\(\\s]([a-dA-D])[.\\)\\s]', exp_text)\n",
    "        correct_mapping = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "\n",
    "        for ans in set(correct_indices): \n",
    "            if ans in correct_mapping:\n",
    "                correct_answers[correct_mapping[ans]] = 1\n",
    "    return correct_answers\n",
    "\n",
    "def preprocess_data(df, label_encoder=None, is_train=True):\n",
    "    df_filtered = df[df['subject_name'].isin(CATEGORIES)].copy()\n",
    "\n",
    "    if is_train:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df_filtered['subject_encoded'] = label_encoder.fit_transform(df_filtered['subject_name'])\n",
    "    else:\n",
    "        if label_encoder is None:\n",
    "            raise ValueError(\"label_encoder must be provided for dev/test data\")\n",
    "        df_filtered['subject_encoded'] = df_filtered['subject_name'].apply(\n",
    "            lambda x: label_encoder.transform([x])[0] if x in label_encoder.classes_ else -1\n",
    "        )\n",
    "        df_filtered = df_filtered[df_filtered['subject_encoded'] != -1]\n",
    "\n",
    "    if 'exp' in df_filtered.columns:\n",
    "        print(\"Found 'exp' column. Parsing answer labels...\")\n",
    "        df_filtered['answer_labels'] = df_filtered.apply(parse_correct_answers, axis=1)\n",
    "    else:\n",
    "        print(\"Warning: 'exp' column not found. Creating dummy answer labels [0,0,0,0].\")\n",
    "        dummy_labels = [[0, 0, 0, 0] for _ in range(len(df_filtered))]\n",
    "        df_filtered['answer_labels'] = dummy_labels\n",
    "\n",
    "    # Create Negation Feature\n",
    "    negation_words = [' except ', ' not ', ' false ', ' incorrect ']\n",
    "    df_filtered['negation_feature'] = df_filtered['question'].apply(\n",
    "        lambda x: 1.0 if any(word in str(x).lower() for word in negation_words) else 0.0\n",
    "    )\n",
    "\n",
    "    if is_train:\n",
    "        return df_filtered, label_encoder\n",
    "    else:\n",
    "        return df_filtered\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "train_df, label_encoder = preprocess_data(train_df_raw, is_train=True)\n",
    "\n",
    "dev_df = preprocess_data(dev_df_raw, label_encoder=label_encoder, is_train=False)\n",
    "test_df = preprocess_data(test_df_raw, label_encoder=label_encoder, is_train=False)\n",
    "\n",
    "print(f\"Filtered data: train({len(train_df)}), dev({len(dev_df)}), test({len(test_df)})\")\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FXy8TV__ATI"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:29.584798Z",
     "iopub.status.busy": "2025-11-10T05:38:29.584497Z",
     "iopub.status.idle": "2025-11-10T05:38:35.147675Z",
     "shell.execute_reply": "2025-11-10T05:38:35.146818Z",
     "shell.execute_reply.started": "2025-11-10T05:38:29.584778Z"
    },
    "id": "BGuDdP3JxzZf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating data visualizations\")\n",
    "\n",
    "print(\"Generating Word Cloud...\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(train_df['question']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Frequent Terms in MedMCQA Questions', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"Generating Correct Option Distribution...\")\n",
    "option_counts = train_df['cop'].value_counts()\n",
    "colors = ['#F05454', '#30475E', '#F9D14B', '#DFFB95']\n",
    "explode = (0.05, 0.05, 0.05, 0.05)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(option_counts,\n",
    "        labels=[f'Option {idx}' for idx in option_counts.index],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=colors,\n",
    "        explode=explode,\n",
    "        shadow=True)\n",
    "plt.title('Correct Option Distribution', fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.legend(option_counts.index, loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "print(\"Generating Subject Category Distribution...\")\n",
    "subject_counts = train_df['subject_name'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "bars = plt.bar(subject_counts.index, subject_counts.values, color='#30475E', width=0.6)\n",
    "plt.xlabel('Subject Name', fontsize=12)\n",
    "plt.ylabel('Number of Questions', fontsize=12)\n",
    "plt.title('Frequency of Questions per Subject Category', fontsize=14)\n",
    "plt.xticks(rotation=75, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom', ha='center', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "colors_12 = ['#FF6F61', '#6B5B93', '#88B04B', '#F7CAC9', '#92A8D1', '#955251',\n",
    "             '#B68D40', '#F6BDC0', '#6A0588', '#F1C40F', '#FF9F00', '#00BFFF']\n",
    "explode_12 = (0.05,) * len(subject_counts)\n",
    "plt.pie(subject_counts,\n",
    "        labels=subject_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90,\n",
    "        colors=colors_12,\n",
    "        explode=explode_12,\n",
    "        shadow=True,\n",
    "        pctdistance=0.85)\n",
    "plt.title('Percentage Distribution of Medical Subjects', fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jL6uHuO_EPP"
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:35.150188Z",
     "iopub.status.busy": "2025-11-10T05:38:35.149911Z",
     "iopub.status.idle": "2025-11-10T05:38:35.159229Z",
     "shell.execute_reply": "2025-11-10T05:38:35.158486Z",
     "shell.execute_reply.started": "2025-11-10T05:38:35.150170Z"
    },
    "id": "SeIVYhg1hmE5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MedicalQADataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_length, is_test=False):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_test = is_test \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = str(self.data.iloc[index]['question'])\n",
    "        negation_feature = torch.tensor(self.data.iloc[index]['negation_feature'], dtype=torch.float)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'negation_feature': negation_feature\n",
    "        }\n",
    "\n",
    "        if not self.is_test:\n",
    "            item['subject_labels'] = torch.tensor(self.data.iloc[index]['subject_encoded'], dtype=torch.long)\n",
    "            item['answer_labels'] = torch.tensor(self.data.iloc[index]['answer_labels'], dtype=torch.float)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A3cStP0_H1o"
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:35.160296Z",
     "iopub.status.busy": "2025-11-10T05:38:35.160043Z",
     "iopub.status.idle": "2025-11-10T05:38:35.338004Z",
     "shell.execute_reply": "2025-11-10T05:38:35.337415Z",
     "shell.execute_reply.started": "2025-11-10T05:38:35.160273Z"
    },
    "id": "hXadxAjHwIY7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_scores = self.attention_weights(hidden_states).squeeze(-1)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, -1e9)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1) \n",
    "        pooled_output = torch.sum(hidden_states * attention_weights.unsqueeze(-1), dim=1)\n",
    "        return pooled_output\n",
    "\n",
    "class BRT_Cell(nn.Module):\n",
    "\n",
    "    def __init__(self, bert_hidden_size, num_states, state_dim, num_heads=8):\n",
    "        super(BRT_Cell, self).__init__()\n",
    "        self.num_states = num_states\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        self.state_vectors = nn.Parameter(torch.randn(num_states, state_dim))\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=state_dim, \n",
    "            num_heads=num_heads, \n",
    "            kdim=bert_hidden_size, \n",
    "            vdim=bert_hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=state_dim, \n",
    "            num_heads=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        combined_dim = state_dim * 2\n",
    "        self.forget_gate = nn.Linear(combined_dim, state_dim)\n",
    "        self.input_gate = nn.Linear(combined_dim, state_dim)\n",
    "        self.candidate_gate = nn.Linear(combined_dim, state_dim)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "\n",
    "        batch_size = hidden_states.size(0)\n",
    "        \n",
    "        states = self.state_vectors.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        padding_mask = (attention_mask == 0)\n",
    "        \n",
    "        cross_attn_out, _ = self.cross_attn(\n",
    "            query=states, \n",
    "            key=hidden_states, \n",
    "            value=hidden_states, \n",
    "            key_padding_mask=padding_mask\n",
    "        )\n",
    "        \n",
    "        self_attn_out, _ = self.self_attn(\n",
    "            query=cross_attn_out, \n",
    "            key=cross_attn_out, \n",
    "            value=cross_attn_out\n",
    "        )\n",
    "        \n",
    "        combined_features = torch.cat((cross_attn_out, self_attn_out), dim=-1)\n",
    "        \n",
    "        f = torch.sigmoid(self.forget_gate(combined_features))\n",
    "        i = torch.sigmoid(self.input_gate(combined_features))\n",
    "        c = torch.tanh(self.candidate_gate(combined_features))\n",
    "        \n",
    "        new_states = (states * f) + (c * i)\n",
    "        \n",
    "        pooled_output = new_states.mean(dim=1)\n",
    "        return pooled_output\n",
    "\n",
    "class MedicalQAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, gru_hidden_size=128, dropout_rate=0.25, \n",
    "                 use_bigru=True, use_negation=True, use_brt=False):\n",
    "        \n",
    "        super(MedicalQAModel, self).__init__()\n",
    "        self.use_bigru = use_bigru\n",
    "        self.use_negation = use_negation\n",
    "        self.use_brt = use_brt\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        if self.use_brt:\n",
    "            print(\"--- Initializing model with BRT_Cell ---\")\n",
    "            brt_state_dim = 256\n",
    "            self.brt_cell = BRT_Cell(\n",
    "                bert_hidden_size=bert_hidden_size,\n",
    "                num_states=64,\n",
    "                state_dim=brt_state_dim\n",
    "            )\n",
    "            current_features_size = brt_state_dim\n",
    "            \n",
    "        elif self.use_bigru:\n",
    "            print(\"--- Initializing model with BiGRU ---\")\n",
    "            self.bigru = nn.GRU(\n",
    "                input_size=bert_hidden_size,\n",
    "                hidden_size=gru_hidden_size,\n",
    "                num_layers=1,\n",
    "                bidirectional=True,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.attention_pooling = AttentionPooling(hidden_size=gru_hidden_size * 2)\n",
    "            current_features_size = gru_hidden_size * 2 # 256\n",
    "            \n",
    "        else:\n",
    "            print(\"--- Initializing model with Attention Pooling Only ---\")\n",
    "            self.attention_pooling = AttentionPooling(hidden_size=bert_hidden_size)\n",
    "            current_features_size = bert_hidden_size\n",
    "            \n",
    "        classifier_input_size = current_features_size\n",
    "        if self.use_negation:\n",
    "            classifier_input_size += 1 \n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.subject_classifier = nn.Linear(classifier_input_size, n_classes)\n",
    "        self.answer_classifier = nn.Linear(classifier_input_size, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, negation_feature):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state \n",
    "\n",
    "        if self.use_brt:\n",
    "            pooled_output = self.brt_cell(hidden_state, attention_mask)\n",
    "        elif self.use_bigru:\n",
    "            features, _ = self.bigru(hidden_state)\n",
    "            pooled_output = self.attention_pooling(features, attention_mask)\n",
    "        else:\n",
    "            pooled_output = self.attention_pooling(hidden_state, attention_mask)\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        if self.use_negation:\n",
    "            negation_f = negation_feature.unsqueeze(1)\n",
    "            combined_output = torch.cat((pooled_output, negation_f), dim=1)\n",
    "        else:\n",
    "            combined_output = pooled_output\n",
    "\n",
    "        subject_logits = self.subject_classifier(combined_output)\n",
    "        answer_logits = self.answer_classifier(combined_output)\n",
    "\n",
    "        return subject_logits, answer_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n0krmhi_Lw3"
   },
   "source": [
    "# Hyperparameters & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:35.339153Z",
     "iopub.status.busy": "2025-11-10T05:38:35.338833Z",
     "iopub.status.idle": "2025-11-10T05:38:35.364659Z",
     "shell.execute_reply": "2025-11-10T05:38:35.364056Z",
     "shell.execute_reply.started": "2025-11-10T05:38:35.339133Z"
    },
    "id": "l6ZE5tgxhosA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 64\n",
    "FOCAL_ALPHA = 0.25\n",
    "FOCAL_GAMMA = 2\n",
    "ANSWER_THRESHOLD = 0.3 # As reported in the paper\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "train_dataset = MedicalQADataset(train_df, tokenizer, MAX_LENGTH, is_test=False)\n",
    "dev_dataset = MedicalQADataset(dev_df, tokenizer, MAX_LENGTH, is_test=False)\n",
    "test_dataset = MedicalQADataset(test_df, tokenizer, MAX_LENGTH, is_test=False) \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created with Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8tMmIc1_ObY"
   },
   "source": [
    "# Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:35.365740Z",
     "iopub.status.busy": "2025-11-10T05:38:35.365407Z",
     "iopub.status.idle": "2025-11-10T05:38:42.381488Z",
     "shell.execute_reply": "2025-11-10T05:38:42.380859Z",
     "shell.execute_reply.started": "2025-11-10T05:38:35.365720Z"
    },
    "id": "XlpkvWnehovH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=FOCAL_ALPHA, gamma=FOCAL_GAMMA, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "loss_fn_subject = FocalLoss().to(device)\n",
    "loss_fn_answers = nn.BCEWithLogitsLoss().to(device) \n",
    "\n",
    "model = MedicalQAModel(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    use_bigru=True,\n",
    "    use_negation=True\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "niwaBFhu_RUA"
   },
   "source": [
    "# Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:42.382535Z",
     "iopub.status.busy": "2025-11-10T05:38:42.382254Z",
     "iopub.status.idle": "2025-11-10T05:38:42.397367Z",
     "shell.execute_reply": "2025-11-10T05:38:42.396474Z",
     "shell.execute_reply.started": "2025-11-10T05:38:42.382506Z"
    },
    "id": "o-jHP-yBz6ad",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn_subject, loss_fn_answers, optimizer, device, scheduler):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_subject_correct = 0\n",
    "    total_answer_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        negation_feature = batch['negation_feature'].to(device)\n",
    "        subject_labels = batch['subject_labels'].to(device)\n",
    "        answer_labels = batch['answer_labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        subject_logits, answer_logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            negation_feature=negation_feature\n",
    "        )\n",
    "\n",
    "        loss_subject = loss_fn_subject(subject_logits, subject_labels)\n",
    "        loss_answers = loss_fn_answers(answer_logits, answer_labels)\n",
    "        loss = loss_subject + loss_answers #\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        subject_preds = torch.argmax(subject_logits, dim=1)\n",
    "        total_subject_correct += (subject_preds == subject_labels).sum().item()\n",
    "\n",
    "        answer_preds = (torch.sigmoid(answer_logits) > 0.5).float()\n",
    "        total_answer_correct += (answer_preds == answer_labels).sum().item()\n",
    "\n",
    "        total_samples += subject_labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    subject_accuracy = total_subject_correct / total_samples\n",
    "    answer_accuracy = total_answer_correct / (total_samples * 4)\n",
    "\n",
    "    return avg_loss, subject_accuracy, answer_accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn_subject, loss_fn_answers, device, threshold=ANSWER_THRESHOLD):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_subject_correct = 0\n",
    "    total_answer_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_subject_preds = []\n",
    "    all_subject_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            negation_feature = batch['negation_feature'].to(device)\n",
    "            subject_labels = batch['subject_labels'].to(device)\n",
    "            answer_labels = batch['answer_labels'].to(device)\n",
    "\n",
    "            subject_logits, answer_logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                negation_feature=negation_feature\n",
    "            )\n",
    "\n",
    "            loss_subject = loss_fn_subject(subject_logits, subject_labels)\n",
    "            loss_answers = loss_fn_answers(answer_logits, answer_labels)\n",
    "            loss = loss_subject + loss_answers\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            subject_preds = torch.argmax(subject_logits, dim=1)\n",
    "            total_subject_correct += (subject_preds == subject_labels).sum().item()\n",
    "\n",
    "            answer_preds = (torch.sigmoid(answer_logits) > threshold).float()\n",
    "            total_answer_correct += (answer_preds == answer_labels).sum().item()\n",
    "\n",
    "            total_samples += subject_labels.size(0)\n",
    "\n",
    "            all_subject_preds.extend(subject_preds.cpu().numpy())\n",
    "            all_subject_labels.extend(subject_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    subject_accuracy = total_subject_correct / total_samples\n",
    "    answer_accuracy = total_answer_correct / (total_samples * 4)\n",
    "\n",
    "    subject_precision = precision_score(all_subject_labels, all_subject_preds, average='weighted', zero_division=0)\n",
    "    subject_recall = recall_score(all_subject_labels, all_subject_preds, average='weighted', zero_division=0)\n",
    "    subject_f1 = f1_score(all_subject_labels, all_subject_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    return avg_loss, subject_accuracy, answer_accuracy, subject_precision, subject_recall, subject_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggo_U05M_USg"
   },
   "source": [
    "# Define Save Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:42.398578Z",
     "iopub.status.busy": "2025-11-10T05:38:42.398363Z",
     "iopub.status.idle": "2025-11-10T05:38:42.425908Z",
     "shell.execute_reply": "2025-11-10T05:38:42.425077Z",
     "shell.execute_reply.started": "2025-11-10T05:38:42.398562Z"
    },
    "id": "LZJURdVr3ehh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BASE_SAVE_PATH = \"model_outputs\" \n",
    "\n",
    "if not os.path.exists(BASE_SAVE_PATH):\n",
    "    os.makedirs(BASE_SAVE_PATH)\n",
    "    print(f\"Created directory: {BASE_SAVE_PATH}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {BASE_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIC6ch-E_eEK"
   },
   "source": [
    "# Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:42.427251Z",
     "iopub.status.busy": "2025-11-10T05:38:42.426822Z",
     "iopub.status.idle": "2025-11-10T05:38:42.451610Z",
     "shell.execute_reply": "2025-11-10T05:38:42.450882Z",
     "shell.execute_reply.started": "2025-11-10T05:38:42.427221Z"
    },
    "id": "YVNFm2o-2N3j",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training_experiment(model, optimizer, scheduler, experiment_name):\n",
    "\n",
    "    print(f\"\\n{'='*20} STARTING EXPERIMENT: {experiment_name} {'='*20}\")\n",
    "\n",
    "    EXPERIMENT_PATH = os.path.join(BASE_SAVE_PATH, experiment_name)\n",
    "    if not os.path.exists(EXPERIMENT_PATH):\n",
    "        os.makedirs(EXPERIMENT_PATH)\n",
    "\n",
    "    BEST_MODEL_PATH = os.path.join(EXPERIMENT_PATH, 'best_model_state.bin')\n",
    "    HISTORY_PATH = os.path.join(EXPERIMENT_PATH, 'training_history.csv')\n",
    "\n",
    "    print(f\"Best model will be saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    best_dev_f1 = 0.0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_subj_acc': [], 'train_ans_acc': [],\n",
    "        'dev_loss': [], 'dev_subj_acc': [], 'dev_ans_acc': [], 'dev_subj_f1': []\n",
    "    }\n",
    "\n",
    "    patience = 3\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_subj_acc, train_answer_acc = train_epoch(\n",
    "            model, train_loader, loss_fn_subject, loss_fn_answers, optimizer, device, scheduler\n",
    "        )\n",
    "\n",
    "        dev_loss, dev_subj_acc, dev_answer_acc, dev_subj_prec, dev_subj_rec, dev_subj_f1 = eval_model(\n",
    "            model, dev_loader, loss_fn_subject, loss_fn_answers, device, threshold=ANSWER_THRESHOLD\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_subj_acc'].append(train_subj_acc)\n",
    "        history['train_ans_acc'].append(train_answer_acc)\n",
    "        history['dev_loss'].append(dev_loss)\n",
    "        history['dev_subj_acc'].append(dev_subj_acc)\n",
    "        history['dev_ans_acc'].append(dev_answer_acc)\n",
    "        history['dev_subj_f1'].append(dev_subj_f1)\n",
    "\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} | Time: {int(epoch_mins)}m {int(epoch_secs)}s ---\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.4f} | Train Subj Acc: {train_subj_acc:.2%} | Train Ans Acc: {train_answer_acc:.2%}\")\n",
    "        print(f\"\\tDev Loss:   {dev_loss:.4f} | Dev Subj Acc:   {dev_subj_acc:.2%} | Dev Ans Acc:   {dev_answer_acc:.2%}\")\n",
    "        print(f\"\\tDev Subject F1: {dev_subj_f1:.4f} (P: {dev_subj_prec:.4f}, R: {dev_subj_rec:.4f})\")\n",
    "\n",
    "        if dev_subj_f1 > best_dev_f1:\n",
    "            best_dev_f1 = dev_subj_f1\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            print(f\"\\t*** New best model saved to Drive with F1: {best_dev_f1:.4f} ***\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"\\tNo improvement in F1 for {epochs_no_improve} epoch(s). Patience: {patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\n--- EARLY STOPPING triggered after {epoch + 1} epochs. ---\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nTraining complete for experiment: {experiment_name}\")\n",
    "    print(f\"Best model state (F1: {best_dev_f1:.4f}) saved to: {BEST_MODEL_PATH}\")\n",
    "\n",
    "    try:\n",
    "        history_df = pd.DataFrame(history)\n",
    "        history_df.to_csv(HISTORY_PATH, index=False)\n",
    "        print(f\"Training history saved to {HISTORY_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving training history: {e}\")\n",
    "\n",
    "    return BEST_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu7JPpmN_gyX"
   },
   "source": [
    "# Analysis & Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:42.452783Z",
     "iopub.status.busy": "2025-11-10T05:38:42.452493Z",
     "iopub.status.idle": "2025-11-10T05:38:42.476667Z",
     "shell.execute_reply": "2025-11-10T05:38:42.475999Z",
     "shell.execute_reply.started": "2025-11-10T05:38:42.452741Z"
    },
    "id": "qcPi-rcfhoyq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_predictions(df, y_pred_subj, y_pred_ans, y_prob_ans, label_encoder, num_samples=5):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  QUALITATIVE PREDICTION SAMPLES\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "\n",
    "    if len(df_reset) > num_samples:\n",
    "        sample_indices = np.random.choice(len(df_reset), num_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = range(len(df_reset))\n",
    "\n",
    "    for i in sample_indices:\n",
    "        row = df_reset.iloc[i]\n",
    "        print(f\"\\n--- Sample ID: {row.get('id', 'N/A')} ---\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "\n",
    "        true_subject = row['subject_name']\n",
    "        pred_subject = label_encoder.inverse_transform([y_pred_subj[i]])[0]\n",
    "        subj_status = \"✓\" if true_subject == pred_subject else \"✗\"\n",
    "        print(f\"Subject: [True: {true_subject}] | [Pred: {pred_subject}] {subj_status}\")\n",
    "\n",
    "        true_answers = row['answer_labels']\n",
    "        pred_answers = y_pred_ans[i]\n",
    "        prob_answers = y_prob_ans[i]\n",
    "\n",
    "        options = ['A', 'B', 'C', 'D']\n",
    "        for j, opt in enumerate(options):\n",
    "            opt_text = row[f'op{opt.lower()}']\n",
    "            status = \"✓\" if int(true_answers[j]) == int(pred_answers[j]) else \"✗\"\n",
    "            pred_mark = f\"Selected (Prob: {prob_answers[j]:.3f})\" if int(pred_answers[j]) == 1 else \"Not Selected\"\n",
    "            true_mark = \"Correct\" if int(true_answers[j]) == 1 else \"Incorrect\"\n",
    "            print(f\"  Opt {opt}: {opt_text}\")\n",
    "            print(f\"    Prediction: {pred_mark} | Actual: {true_mark} | Status: {status}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "\n",
    "def plot_subject_roc(y_true, y_pred, labels, save_path=None):\n",
    "\n",
    "    print(\"Generating Subject Classification ROC Curve\")\n",
    "\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(labels)))\n",
    "    y_pred_bin = label_binarize(y_pred, classes=range(len(labels)))\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, label=f'{labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Subject Classification ROC Curve (Per-Class)', fontsize=14)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"ROC curve saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYItN2Q7_ok_"
   },
   "source": [
    "# Final Test Set Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T05:38:42.477832Z",
     "iopub.status.busy": "2025-11-10T05:38:42.477569Z",
     "iopub.status.idle": "2025-11-10T05:38:42.503287Z",
     "shell.execute_reply": "2025-11-10T05:38:42.502403Z",
     "shell.execute_reply.started": "2025-11-10T05:38:42.477806Z"
    },
    "id": "Jpn8AxAW9zwD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_test_predictions_and_labels(model, data_loader, device, threshold=ANSWER_THRESHOLD):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_subject_preds = []\n",
    "    all_subject_labels = []\n",
    "    all_answer_preds = []\n",
    "    all_answer_labels = []\n",
    "    all_answer_probs = []\n",
    "\n",
    "    total_inference_time = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            negation_feature = batch['negation_feature'].to(device)\n",
    "            subject_labels = batch['subject_labels'].to(device)\n",
    "            answer_labels = batch['answer_labels'].to(device)\n",
    "\n",
    "            start_batch_time = time.time()\n",
    "\n",
    "            subject_logits, answer_logits = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                negation_feature=negation_feature\n",
    "            )\n",
    "\n",
    "            end_batch_time = time.time()\n",
    "            total_inference_time += (end_batch_time - start_batch_time)\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "            subject_preds = torch.argmax(subject_logits, dim=1)\n",
    "            all_subject_preds.extend(subject_preds.cpu().numpy())\n",
    "            all_subject_labels.extend(subject_labels.cpu().numpy())\n",
    "\n",
    "            answer_probs_batch = torch.sigmoid(answer_logits)\n",
    "            answer_preds = (answer_probs_batch > threshold).float()\n",
    "\n",
    "            all_answer_preds.extend(answer_preds.cpu().numpy())\n",
    "            all_answer_labels.extend(answer_labels.cpu().numpy())\n",
    "            all_answer_probs.extend(answer_probs_batch.cpu().numpy())\n",
    "\n",
    "    avg_inference_time_ms = (total_inference_time / total_samples) * 1000\n",
    "\n",
    "    return (\n",
    "        np.array(all_subject_labels), np.array(all_subject_preds),\n",
    "        np.array(all_answer_labels), np.array(all_answer_preds),\n",
    "        np.array(all_answer_probs),\n",
    "        avg_inference_time_ms\n",
    "    )\n",
    "\n",
    "\n",
    "def run_final_evaluation(model, best_model_path, experiment_name, labels, test_df_for_display):\n",
    "\n",
    "    print(f\"\\n{'='*20} STARTING FINAL EVALUATION: {experiment_name} {'='*20}\")\n",
    "\n",
    "    EXPERIMENT_PATH = os.path.join(BASE_SAVE_PATH, experiment_name)\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        print(f\"Successfully loaded best model from {best_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model state: {e}. Aborting evaluation.\")\n",
    "        return\n",
    "\n",
    "    print(\"Running predictions on the test set...\")\n",
    "    y_true_subj, y_pred_subj, y_true_ans, y_pred_ans, y_prob_ans, avg_inference_time_ms = get_test_predictions_and_labels(\n",
    "        model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        threshold=ANSWER_THRESHOLD\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"  FINAL TEST SET METRICS\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    report_lines = [] \n",
    "\n",
    "    report_lines.append(\"--- Practical Deployment Metrics ---\")\n",
    "    report_lines.append(f\"Average Inference Time per Sample: {avg_inference_time_ms:.2f} ms\")\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "    overall_subject_accuracy = np.mean(y_true_subj == y_pred_subj)\n",
    "    subject_precision = precision_score(y_true_subj, y_pred_subj, average='weighted', zero_division=0)\n",
    "    subject_recall = recall_score(y_true_subj, y_pred_subj, average='weighted', zero_division=0)\n",
    "    subject_f1 = f1_score(y_true_subj, y_pred_subj, average='weighted', zero_division=0)\n",
    "\n",
    "    report_lines.append(\"--- Overall Subject Classification ---\")\n",
    "    report_lines.append(f\"Overall Subject Accuracy: {overall_subject_accuracy:.4f}\")\n",
    "    report_lines.append(f\"Overall Subject Precision: {subject_precision:.4f}\")\n",
    "    report_lines.append(f\"Overall Subject Recall: {subject_recall:.4f}\")\n",
    "    report_lines.append(f\"Overall Subject F1-Score: {subject_f1:.4f}\")\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "    answer_accuracy = np.mean(y_true_ans == y_pred_ans)\n",
    "    report_lines.append(\"--- Overall Answer Prediction Metrics ---\")\n",
    "    report_lines.append(f\"Overall Answer Accuracy (Label-Based): {answer_accuracy:.4f}\")\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "    report_lines.append(\"--- Per-Class Subject Classification Report ---\")\n",
    "    class_report = classification_report(y_true_subj, y_pred_subj, target_names=labels, zero_division=0)\n",
    "    report_lines.append(class_report)\n",
    "    report_lines.append(\"\\n\")\n",
    "\n",
    "    for line in report_lines:\n",
    "        print(line)\n",
    "\n",
    "    report_path = os.path.join(EXPERIMENT_PATH, 'final_metrics_report.txt')\n",
    "    try:\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(report_lines))\n",
    "        print(f\"\\nFinal metrics report saved to {report_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metrics report to Drive: {e}\")\n",
    "\n",
    "    print(\"\\nGenerating Confusion Matrix...\")\n",
    "    try:\n",
    "        conf_matrix = confusion_matrix(y_true_subj, y_pred_subj)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f'Confusion Matrix - {experiment_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        cm_path = os.path.join(EXPERIMENT_PATH, 'confusion_matrix.png')\n",
    "        plt.savefig(cm_path)\n",
    "        plt.show()\n",
    "        print(f\"Confusion matrix saved to {cm_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating confusion matrix: {e}\")\n",
    "\n",
    "    try:\n",
    "        roc_path = os.path.join(EXPERIMENT_PATH, 'roc_curve.png')\n",
    "        plot_subject_roc(y_true_subj, y_pred_subj, labels=labels, save_path=roc_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating ROC curve: {e}\")\n",
    "\n",
    "    try:\n",
    "        display_predictions(\n",
    "            test_df_for_display, y_pred_subj, y_pred_ans, y_prob_ans, label_encoder, num_samples=5\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying qualitative samples: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI_OHG0p_3iH"
   },
   "source": [
    "# Run Ablation Study Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-10T05:47:59.398Z",
     "iopub.execute_input": "2025-11-10T05:38:42.506152Z",
     "iopub.status.busy": "2025-11-10T05:38:42.505596Z"
    },
    "id": "-odiefna5mUu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "print(\"--- LAUNCHING EXPERIMENT 1: Base_Model_BioBERT_Attention ---\")\n",
    "model_base = MedicalQAModel(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    use_bigru=False,\n",
    "    use_negation=False,\n",
    "    use_brt=False\n",
    ").to(device)\n",
    "\n",
    "optimizer_base = AdamW(model_base.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler_base = get_linear_schedule_with_warmup(optimizer_base, 0, total_steps)\n",
    "\n",
    "best_model_path_base = run_training_experiment(\n",
    "    model_base, optimizer_base, scheduler_base, \"Base_Model_BioBERT_Attention\"\n",
    ")\n",
    "print(\"--- FINISHED EXPERIMENT 1 ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- LAUNCHING EXPERIMENT 2: BioBERT_BiGRU_Model_No_Feature ---\")\n",
    "model_bigru = MedicalQAModel(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    use_bigru=True,\n",
    "    use_negation=False,\n",
    "    use_brt=False\n",
    ").to(device)\n",
    "\n",
    "optimizer_bigru = AdamW(model_bigru.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler_bigru = get_linear_schedule_with_warmup(optimizer_bigru, 0, total_steps)\n",
    "\n",
    "best_model_path_bigru = run_training_experiment(\n",
    "    model_bigru, optimizer_bigru, scheduler_bigru, \"BioBERT_BiGRU_Model_No_Feature\"\n",
    ")\n",
    "print(\"--- FINISHED EXPERIMENT 2 ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- LAUNCHING EXPERIMENT 3: BioBERT_BiGRU_Model_with_Negation ---\")\n",
    "model_bigru_neg = MedicalQAModel(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    use_bigru=True,\n",
    "    use_negation=True,\n",
    "    use_brt=False\n",
    ").to(device)\n",
    "\n",
    "optimizer_bigru_neg = AdamW(model_bigru_neg.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler_bigru_neg = get_linear_schedule_with_warmup(optimizer_bigru_neg, 0, total_steps)\n",
    "\n",
    "best_model_path_bigru_neg = run_training_experiment(\n",
    "    model_bigru_neg, optimizer_bigru_neg, scheduler_bigru_neg, \"BioBERT_BiGRU_Model_with_Negation\"\n",
    ")\n",
    "print(\"--- FINISHED EXPERIMENT 3 ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- LAUNCHING EXPERIMENT 4: BRT_Model_with_Negation ---\")\n",
    "model_brt_neg = MedicalQAModel(\n",
    "    n_classes=NUM_CLASSES,\n",
    "    use_bigru=False,\n",
    "    use_negation=True,\n",
    "    use_brt=True \n",
    ").to(device)\n",
    "\n",
    "optimizer_brt_neg = AdamW(model_brt_neg.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler_brt_neg = get_linear_schedule_with_warmup(optimizer_brt_neg, 0, total_steps)\n",
    "\n",
    "best_model_path_brt_neg = run_training_experiment(\n",
    "    model_brt_neg, optimizer_brt_neg, scheduler_brt_neg, \"BRT_Model_with_Negation\"\n",
    ")\n",
    "print(\"--- FINISHED EXPERIMENT 4 ---\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"  ALL EXPERIMENTS COMPLETE. RUNNING FINAL EVALUATIONS ON TEST SET...\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Evaluating Base_Model_BioBERT_Attention ---\")\n",
    "model_base_eval = MedicalQAModel(n_classes=NUM_CLASSES, use_bigru=False, use_negation=False, use_brt=False).to(device)\n",
    "run_final_evaluation(\n",
    "    model_base_eval,\n",
    "    best_model_path_base,\n",
    "    \"Base_Model_BioBERT_Attention\",\n",
    "    labels=label_encoder.classes_,\n",
    "    test_df_for_display=test_df\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating BioBERT-BiGRU_Model_No_Feature ---\")\n",
    "model_bigru_eval = MedicalQAModel(n_classes=NUM_CLASSES, use_bigru=True, use_negation=False, use_brt=False).to(device)\n",
    "run_final_evaluation(\n",
    "    model_bigru_eval,\n",
    "    best_model_path_bigru,\n",
    "    \"BioBERT_BiGRU_Model_No_Feature\",\n",
    "    labels=label_encoder.classes_,\n",
    "    test_df_for_display=test_df\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating BioBERT-BiGRU_Model_with_Negation ---\")\n",
    "model_bigru_neg_eval = MedicalQAModel(n_classes=NUM_CLASSES, use_bigru=True, use_negation=True, use_brt=False).to(device)\n",
    "run_final_evaluation(\n",
    "    model_bigru_neg_eval,\n",
    "    best_model_path_bigru_neg,\n",
    "    \"BioBERT_BiGRU_Model_with_Negation\",\n",
    "    labels=label_encoder.classes_,\n",
    "    test_df_for_display=test_df\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating BRT_Model_with_Negation ---\")\n",
    "model_brt_neg_eval = MedicalQAModel(n_classes=NUM_CLASSES, use_bigru=False, use_negation=True, use_brt=True).to(device)\n",
    "run_final_evaluation(\n",
    "    model_brt_neg_eval,\n",
    "    best_model_path_brt_neg,\n",
    "    \"BRT_Model_with_Negation\",\n",
    "    labels=label_encoder.classes_,\n",
    "    test_df_for_display=test_df\n",
    ")\n",
    "\n",
    "print(\"\\n\\n--- FULL REVISION EXPERIMENTS & FINAL EVALUATION COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8675220,
     "sourceId": 13646624,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
